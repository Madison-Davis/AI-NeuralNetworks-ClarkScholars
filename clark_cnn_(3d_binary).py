# -*- coding: utf-8 -*-
"""Clark: CNN (3D Binary).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rKfZqpAcwcvOKQaKO9rMqpgBKx29gYWM
"""

# DATASETS:
  # COVID-CT MASTER
  # COVID-19 CT

# IMPORTS
from PIL import Image   
from numpy import asarray
from sklearn.model_selection import train_test_split
from sklearn import metrics
from google.colab import auth
from oauth2client.client import GoogleCredentials
from skimage import data, img_as_float
from skimage import exposure

import numpy as np
import os
import matplotlib.pyplot as plt
import tensorflow as tf
import gspread

auth.authenticate_user()
gc = gspread.authorize(GoogleCredentials.get_application_default())

# COVID-CT MASTER DATASET
# COVID: 
  # 1-349, SO 349 IMAGES TOTAL ^
  # PRESUMED NUM COVID PATIENTS: 216, STARTS AT 1, BUT...
  # THERE ARE ACTUALLY 213 PATIENTS: DATA SKIPS RANDOM NUMBERS
    # EX: PATIENT 76 TO PATIENT 78 (SKIPS 1)
    # EX2: PATIENT 78 TO PATIENT 80 (SKIPS 1)
  # IN DATA: ROWS 2-350 HAVE PATIENT IDs
  # IN CODE: ROWS 1-349 HAVE PATIENT IDs, ROW 350 IS EMPTY CELL
# NONCOVID: 
  # 1-397, SO 397 IMAGES TOTAL ^
  # PRESUMED NUM COVID PATIENTS: 175, STARTS AT 0, BUT...
  # THERE ARE ACTUALLY 170 PATIENTS: DATA SKIPS RANDOM NUMBERS
    # EX: PATIENT 22 TO PATIENT 25 (SKIPS 2)
  # IN DATA: ROWS 2-398 HAVE PATIENT IDs
  # IN CODE: ROWS 1-397 HAVE PATIENT IDs
# COVID SCANS    = 213 STACKED CT SCANS FOR EACH PATIENT
# NONCOVID SCANS = 170 STACKED CT SCANS FOR EACH PATIENT

# VARIABLES
COVID_data= gc.open("COVID-Metadata").sheet1
NONCOVID_data= gc.open("Non-COVID-Metadata").sheet1
COVID_images = "//content//drive/MyDrive//Programs, Internships//Program: Clark Scholars//Data//BinaryClass//COVID-CT Master//COVID"
NONCOVID_images = "//content//drive/MyDrive//Programs, Internships//Program: Clark Scholars//Data//BinaryClass//COVID-CT Master//Non-COVID"
rows = COVID_data.get_all_values()
rows2 = NONCOVID_data.get_all_values()

# COVID-CT MASTER DATASET
# COVID IMAGES
COVID_CT_MASTER_scans = []

i = 1 # START AT 1 TO AVOID TITLES
single_scan = []
while i < 349:
  if i < 348:
    j = i + 1
  else:
    i = 349
    j = 1
  if rows[i][1] == rows[j][1]: # THE NEXT IMAGE (J) IS THE SAME PATIENT (I): JUST ADD
    for image in os.scandir(COVID_images):
      if image.name == rows[i][0]: # IMAGE ID CORRESPONDS TO EACH PATIENT
        scan_layer = asarray(Image.open(str(image.path), mode = 'r'))   
        # DATA RESCALING
        #p2, p98 = np.percentile(scan_layer, (2, 98))
        #scan_layer = exposure.rescale_intensity(scan_layer, in_range=(p2, p98)) 
        # DATA RESCALING
        mean = scan_layer.astype(np.float32).mean()
        std = scan_layer.astype(np.float32).std()
        scan_layer = (scan_layer - mean) / std
        mean, std = scan_layer.mean(), scan_layer.std()
        # DATA RESIZING
        scan_layer = np.resize(scan_layer, (200,200,3))                      
        single_scan.append(scan_layer)
  else: # LAST LAYER OF PATIENT SCAN
    for image in os.scandir(COVID_images):
      if image.name == rows[i][0]: 
        scan_layer = asarray(Image.open(str(image.path), mode = 'r'))
        # DATA RESCALING
        #p2, p98 = np.percentile(scan_layer, (2, 98))
        #scan_layer = exposure.rescale_intensity(scan_layer, in_range=(p2, p98)) 
        # DATA RESCALING
        mean = scan_layer.astype(np.float32).mean()
        std = scan_layer.astype(np.float32).std()
        scan_layer = (scan_layer - mean) / std
        mean, std = scan_layer.mean(), scan_layer.std()
        # DATA RESIZING
        scan_layer = np.resize(scan_layer, (200,200,3))      
        single_scan.append(scan_layer)
    COVID_CT_MASTER_scans.append(asarray(single_scan))
    single_scan = []
  i = i + 1
i = 0
ct_scans = asarray(COVID_CT_MASTER_scans)
COVID_CT_MASTER_scans = []
for scan in ct_scans:
  scan = np.resize(scan, (3, 200, 200, 3))
  COVID_CT_MASTER_scans.append(scan)
  i = i + 1

# COVID-CT MASTER DATASET
# NON-COVID IMAGES
NONCOVID_CT_MASTER_scans = []

i = 1 # START AT 1 TO AVOID TITLES
single_scan = []
while i < 398: # STOP AT 397
  if i < 397:
    j = i + 1
  else: # THEN I = 397
    j = 396 # SET TO 396 SO THAT WE CAN GET LAST PATIENT
  if rows2[i][4] == rows2[j][4]:
    for image in os.scandir(NONCOVID_images):
      if image.name == rows2[i][3]: # IMAGE ID CORRESPONDS TO EACH PATIENT
        scan_layer = asarray(Image.open(str(image.path), mode = 'r'))     
        # DATA RESCALING
        #p2, p98 = np.percentile(scan_layer, (2, 98))
        #scan_layer = exposure.rescale_intensity(scan_layer, in_range=(p2, p98))   
        # DATA RESCALING
        mean = scan_layer.astype(np.float32).mean()
        std = scan_layer.astype(np.float32).std()
        scan_layer = (scan_layer - mean) / std
        mean, std = scan_layer.mean(), scan_layer.std()
        # DATA RESIZING
        scan_layer = np.resize(scan_layer, (200,200,3))        
        single_scan.append(scan_layer)
  else: # LAST LAYER OF PATIENT SCAN
    for image in os.scandir(NONCOVID_images):
      if image.name == rows2[i][3]: 
        scan_layer = asarray(Image.open(str(image.path), mode = 'r'))
        # DATA RESCALING
        #p2, p98 = np.percentile(scan_layer, (2, 98))
        #scan_layer = exposure.rescale_intensity(scan_layer, in_range=(p2, p98))
        # DATA RESCALING
        mean = scan_layer.astype(np.float32).mean()
        std = scan_layer.astype(np.float32).std()
        scan_layer = (scan_layer - mean) / std
        mean, std = scan_layer.mean(), scan_layer.std()
        # DATA RESIZING
        scan_layer = np.resize(scan_layer, (200,200,3))  
        single_scan.append(scan_layer)
    NONCOVID_CT_MASTER_scans.append(asarray(single_scan))
    single_scan = []
  i = i + 1
i = 0
ct_scans = asarray(NONCOVID_CT_MASTER_scans)
NONCOVID_CT_MASTER_scans = []
for scan in ct_scans:
  if scan.shape != (0,):
    scan = np.resize(scan, (3, 200, 200, 3))
    NONCOVID_CT_MASTER_scans.append(scan)
  i = i + 1

# COVID-19 CT DATASET 
# COVID: 
  # IN DATA: ROWS 2-667 HAVE PATIENT IDs
  # IN CODE: ROWS 1-666 HAVE PATIENT IDs
# NONCOVID: 
  # IN DATA: ROWS 2-325 HAVE PATIENT IDs 
  # IN CODE: ROWS 1-324 HAVE PATIENT IDs (GOING UP TO PATIENT 350)
# COVID SCANS    = 68 STACKED CT SCANS FOR EACH PATIENT
# NONCOVID SCANS = 107 STACKED CT SCANS FOR EACH PATIENT (GOING UP TO PATIENT 350)

# VARIABLES
COVID_data= gc.open("COVID2-Metadata").sheet1
NONCOVID_data= gc.open("Non-COVID2-Metadata").sheet1
COVID_images = "//content//drive/MyDrive//Programs, Internships//Program: Clark Scholars//Data//BinaryClass//COVID-19 CT//COVID"
NONCOVID_images = "//content//drive/MyDrive//Programs, Internships//Program: Clark Scholars//Data//BinaryClass//COVID-19 CT//Non-COVID"
rows3 = COVID_data.get_all_values()
rows4 = NONCOVID_data.get_all_values()

# COVID-19 CT DATASET 
# COVID IMAGES
COVID_CT_scans = []

i = 1 # START AT 1 TO AVOID TITLES
single_scan = []
while i < 667:
  if i < 666:
    j = i + 1
  else:
    j = 1
  if rows3[i][2] == rows3[j][2]: # THE NEXT IMAGE (J) IS THE SAME PATIENT (I): JUST ADD
    for image in os.scandir(COVID_images):
      if image.name == rows3[i][1]: # IMAGE ID CORRESPONDS TO EACH PATIENT
        scan_layer = asarray(Image.open(str(image.path), mode = 'r'))   
        # DATA RESCALING
        #p2, p98 = np.percentile(scan_layer, (2, 98))
        #scan_layer = exposure.rescale_intensity(scan_layer, in_range=(p2, p98))
        # DATA RESCALING
        mean = scan_layer.astype(np.float32).mean()
        std = scan_layer.astype(np.float32).std()
        scan_layer = (scan_layer - mean) / std
        mean, std = scan_layer.mean(), scan_layer.std()
        # DATA RESIZING
        scan_layer = np.resize(scan_layer, (200,200,3))                      
        single_scan.append(scan_layer)
  else: # LAST LAYER OF PATIENT SCAN
    for image in os.scandir(COVID_images):
      if image.name == rows3[i][1]: 
        scan_layer = asarray(Image.open(str(image.path), mode = 'r'))
        # DATA RESCALING
        #p2, p98 = np.percentile(scan_layer, (2, 98))
        #scan_layer = exposure.rescale_intensity(scan_layer, in_range=(p2, p98))
        # DATA RESCALING
        mean = scan_layer.astype(np.float32).mean()
        std = scan_layer.astype(np.float32).std()
        scan_layer = (scan_layer - mean) / std
        mean, std = scan_layer.mean(), scan_layer.std()
        # DATA RESIZING
        scan_layer = np.resize(scan_layer, (200,200,3))        
        single_scan.append(scan_layer)
    COVID_CT_scans.append(asarray(single_scan))
    single_scan = []
  i = i + 1
i = 0
ct_scans = asarray(COVID_CT_scans)
COVID_CT_scans = []
for scan in ct_scans:
  scan = np.resize(scan, (3, 200, 200, 3))
  COVID_CT_scans.append(scan)
  i = i + 1

# COVID-19 CT DATASET
# NON-COVID IMAGES
NONCOVID_CT_scans = []

i = 1 # START AT 1 TO AVOID TITLES
single_scan = []
while i < 325:
  if i < 324:
    j = i + 1
  else: 
    j = 1
  if rows4[i][2] == rows4[j][2]:
    for image in os.scandir(NONCOVID_images):
      if image.name == rows4[i][1]: # IMAGE ID CORRESPONDS TO EACH PATIENT
        scan_layer = asarray(Image.open(str(image.path), mode = 'r'))       
        # DATA RESCALING
        #p2, p98 = np.percentile(scan_layer, (2, 98))
        #scan_layer = exposure.rescale_intensity(scan_layer, in_range=(p2, p98)) 
        # DATA RESCALING
        mean = scan_layer.astype(np.float32).mean()
        std = scan_layer.astype(np.float32).std()
        scan_layer = (scan_layer - mean) / std
        mean, std = scan_layer.mean(), scan_layer.std()
        # DATA RESIZING
        scan_layer = np.resize(scan_layer, (200,200,3))       
        single_scan.append(scan_layer)
  else: # LAST LAYER OF PATIENT SCAN
    for image in os.scandir(NONCOVID_images):
      if image.name == rows4[i][1]: 
        scan_layer = asarray(Image.open(str(image.path), mode = 'r'))
        # DATA RESCALING
        #p2, p98 = np.percentile(scan_layer, (2, 98))
        #scan_layer = exposure.rescale_intensity(scan_layer, in_range=(p2, p98))
        # DATA RESCALING
        mean = scan_layer.astype(np.float32).mean()
        std = scan_layer.astype(np.float32).std()
        scan_layer = (scan_layer - mean) / std
        mean, std = scan_layer.mean(), scan_layer.std()
        # DATA RESIZING
        scan_layer = np.resize(scan_layer, (200,200,3))  
        single_scan.append(scan_layer)
    NONCOVID_CT_scans.append(asarray(single_scan))
    single_scan = []
  i = i + 1
i = 0
ct_scans = asarray(NONCOVID_CT_scans)
NONCOVID_CT_scans = []
for scan in ct_scans:
  if scan.shape != (0,):
    scan = np.resize(scan, (3, 200, 200, 3))
    NONCOVID_CT_scans.append(scan)
  i = i + 1

# COMBINED DATASETS


# LABELS
# 1 = COVID
# 0 = NONCOVID
labels = np.ones(len(COVID_CT_scans))             
labels = np.append(labels, np.ones(len(COVID_CT_MASTER_scans)))
labels = np.append(labels, np.zeros(len(NONCOVID_CT_scans)))             
labels = np.append(labels, np.zeros(len(NONCOVID_CT_MASTER_scans)))


# IMAGES
# COVID: 213 + 66 = 281
# NONCOVID: 170 + 107 = 277
# TOTAL: 281 + 277 = 558
images = []
for scan in COVID_CT_scans:
  #scan = np.resize(scan, (3, 200, 200, 3))
  images.append(asarray(scan))
for scan in COVID_CT_MASTER_scans:
  #scan = np.resize(scan, (3, 200, 200, 3))
  images.append(asarray(scan))
for scan in NONCOVID_CT_scans:
  #scan = np.resize(scan, (3, 200, 200, 3))
  images.append(asarray(scan))
for scan in NONCOVID_CT_MASTER_scans:
  #scan = np.resize(scan, (3, 200, 200, 3))
  images.append(asarray(scan))
print("COVID CT:", len(COVID_CT_scans))
print("COVID CT MASTER:", len(COVID_CT_MASTER_scans))
print("NONCOVID CT:", len(NONCOVID_CT_scans))
print("NONCOVID CT MASTER:", len(NONCOVID_CT_MASTER_scans))
print("TOTAL CT SCANS:", len(images))

# TRAINING DATA AND PARAMETERS                   

x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size = 0.2, random_state=42)

x_train = asarray(x_train).astype(np.float32)
x_test = asarray(x_test).astype(np.float32)
y_train = asarray(y_train).astype(np.float32)
y_test = asarray(y_test).astype(np.float32)

y_train = tf.convert_to_tensor(y_train)
y_test = tf.convert_to_tensor(y_test)
x_train = tf.convert_to_tensor(x_train)
x_test = tf.convert_to_tensor(x_test)

# VARIABLES
num_batch_size = 32
activate_func = "relu" 
num_epochs = 10

# MODEL
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.InputLayer(input_shape = (3, 200, 200, 3)))
model.add(tf.keras.layers.Conv3D(32, kernel_size = (3,3,3)))
model.add(tf.keras.layers.Conv3D(20, kernel_size = (1,2,2), activation=activate_func))
model.add(tf.keras.layers.MaxPooling3D(pool_size = 1))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(64, activation= activate_func, kernel_regularizer=tf.keras.regularizers.l2(l=0.1)))
model.add(tf.keras.layers.Dense(1, activation = "sigmoid"))
model.compile(optimizer='SGD', loss = tf.keras.losses.BinaryCrossentropy(), metrics=["accuracy"])
history = model.fit(x_train, y_train, batch_size = num_batch_size, epochs = num_epochs, validation_data = (x_test, y_test))

# CONFUSION MATRIX
def confusion_matrix_sample(cnf_matrix):
  FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  
  FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)
  TP = np.diag(cnf_matrix)
  TN = cnf_matrix.sum() - (FP + FN + TP)
  FP = np.sum(FP)
  FN = np.sum(FN)
  TP = np.sum(TP)
  TN = np.sum(TN)
  S = (TN/(TN + FP))
  print(TN + FP)
  return FP, FN, TP, TN, S


# EVALUATING MODEL
model.summary()
yhat_probs = model.predict(x_test, verbose=0)
yhat_classes = model.predict_classes(x_test, verbose=0)
accuracy = metrics.accuracy_score(y_test, yhat_classes)
precision = metrics.precision_score(y_test, yhat_classes, average = "micro")
recall = metrics.recall_score(y_test, yhat_classes, average = "micro")
f1 = metrics.f1_score(y_test, yhat_classes, average = "micro")
fp, fn, tp, tn, specificity = confusion_matrix_sample(metrics.confusion_matrix(y_test, yhat_classes))

print('\nAccuracy: %f' % accuracy)
print('\nPrecision: %f' % precision)
print('\nSensitivity: %f' % recall)
print('\nF1 score: %f' % f1)
print("\nTN: ", tn, ", FP:", fp, " FN:", fn, " TP:", tp)
print('\nSpecificity:', specificity)

# GRAPHS: METRICS AND CONFUSION MATRIX
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

confusion_matrix_graph = metrics.confusion_matrix(y_test, yhat_classes)
plt.imshow(confusion_matrix_graph, cmap=plt.cm.Reds)
plt.xlabel("Predicted labels")
plt.ylabel("True labels")
plt.xticks([], [])
plt.yticks([], [])
plt.title('Confusion matrix ')
plt.colorbar()
plt.show()

# OTHER CODE FOR COPYING AND PASTING

# KERAS 3D MODEL
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.InputLayer(input_shape = (3, 200, 200, 3)))
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.MaxPool3D(pool_size=2))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.MaxPool3D(pool_size=2))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Conv3D(filters=128, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.MaxPool3D(pool_size=2))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Conv3D(filters=256, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.MaxPool3D(pool_size=2))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.GlobalAveragePooling3D())
model.add(tf.keras.layers.Dense(units=512, activation="relu"))
model.add(tf.keras.layers.Dropout(0.3))
model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))
model.compile(optimizer='adam', loss = tf.keras.losses.BinaryCrossentropy(), metrics=["accuracy"])
model.fit(x_train, y_train, batch_size = num_batch_size, epochs = num_epochs, validation_data = (x_test, y_test))

# ALEX NET 3D MODEL
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.InputLayer(input_shape = (3, 200, 200, 3)))
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, , strides = 4, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPool3D(pool_size=2))
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPool3D(pool_size=2))
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPool3D(pool_size=2))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(units=512, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dropout(0.3))
model.add(tf.keras.layers.Dense(units=512, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dropout(0.3))
model.compile(optimizer='adam', loss = tf.keras.losses.BinaryCrossentropy(), metrics=["accuracy"])
model.fit(x_train, y_train, batch_size = num_batch_size, epochs = num_epochs, validation_data = (x_test, y_test))

# LUNA 3D MODEL
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.InputLayer(input_shape = (3, 200, 200, 3)))
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, , strides = 4, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())

model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.MaxPool3D(pool_size=2))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPool3D(pool_size=2))

model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())

model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPool3D(pool_size=2))

model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.Dense(units=512, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPool3D(pool_size=2))

model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(units=512, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dropout(0.3))
model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

model.compile(optimizer='adam', loss = tf.keras.losses.BinaryCrossentropy(), metrics=["accuracy"])
model.fit(x_train, y_train, batch_size = num_batch_size, epochs = num_epochs, validation_data = (x_test, y_test))

# deCovNet MODEL
def resblock(x, kernelsize, filters):
    fx = layers.Conv2D(filters, kernelsize, activation='relu', padding='same')(x)
    fx = layers.BatchNormalization()(fx)
    fx = layers.Conv2D(filters, kernelsize, padding='same')(fx)
    out = layers.Add()([x,fx])
    out = layers.ReLU()(out)
    out = layers.BatchNormalization()(out)
    return out
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.InputLayer(input_shape = (3, 200, 200, 3)))
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=(5,5,5), activation="relu"))
model.add(resblock("relu", 3, 64))
model.add(tf.keras.layers.MaxPool3D(pool_size=2))
model.add(resblock("relu", 3, 128))
model.add(tf.keras.layers.GlobalMaxPooling3D()) # Changed from "Adaptive Max Pooling"
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.MaxPool3D(pool_size=2))
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.GlobalMaxPooling3D())
model.add(tf.keras.layers.Dense(units=32, activation="sigmoid"))
model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

model.add(tf.keras.layers.InputLayer(input_shape = (3, 200, 200, 3)))
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPool3D(pool_size=1))
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=1, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPool3D(pool_size=1))
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=1, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=1, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Conv3D(filters=64, kernel_size=1, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPool3D(pool_size=1))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(units=512, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dropout(0.3))
model.add(tf.keras.layers.Dense(units=512, activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dropout(0.3))
model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))